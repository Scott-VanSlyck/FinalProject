[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "ST 558: Final Project (EDA)",
    "section": "",
    "text": "Loading required package: tidyverse\n\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tibble' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n[[1]]\nNULL"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "ST 558: Final Project (EDA)",
    "section": "Introduction",
    "text": "Introduction\nFor this final project, we will be working with a dataset from the Diabetes Health Indicators with a response variable indicating whether a patient doesn’t have diabetes or has diabetes. The variables that we will be investigating in comparison to the response variable are shown below:\n\nHighBP: High Blood Pressure\n\n0 = not high\n1 = high BP\n\nBMI: Body Mass Index\nSmoker\n\n0 = &gt;100 cigarettes smoked in your entire life\n1 = &lt;100 cigarettes smoked in your entire life\n\nPhysActivity\n\n0 = no physical activity in the past 30 days\n1 = yes physical activity in the past 30 days\n\nVeggies: Consume vegetables 1 or more times per day\n\n0 = no\n1 = yes\n\nSex\n\n0 = female\n1 = male\n\nAge: 13-level age category\n\n1 = 18-24\n9 = 60-64\n13 = 80 or older\n\nIncome: 8-level scale\n\n1 = less than $10,000\n5 = less than $35,000\n8 = $75,000 or more"
  },
  {
    "objectID": "EDA.html#purpose-of-eda-and-ultimate-goal-of-modeling",
    "href": "EDA.html#purpose-of-eda-and-ultimate-goal-of-modeling",
    "title": "ST 558: Final Project (EDA)",
    "section": "Purpose of EDA and Ultimate Goal of Modeling",
    "text": "Purpose of EDA and Ultimate Goal of Modeling\nThe purpose of our Exploratory Data Analysis (EDA) is to understand the underlying patterns and relationships in the dataset, particularly how the different variables relate to the presence or absence of diabetes. Through EDA, we aim to:\n\nIdentify and handle missing data\nDetect outliers and understand their impact\nExplore the distribution of variables\nAnalyze correlations between variables\n\nThe ultimate goal of our modeling is to develop a predictive model that can accurately classify individuals into one of three categories: no diabetes, prediabetes, or diabetes. By understanding the indicators and risks associated with diabetes, we hope to create a model that can assist healthcare professionals in early detection and intervention, potentially improving patient outcomes."
  },
  {
    "objectID": "EDA.html#data",
    "href": "EDA.html#data",
    "title": "ST 558: Final Project (EDA)",
    "section": "Data",
    "text": "Data\nReading in the data using a relative path and manipulating it\n\n# reading in the csv file\nDBH &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# selecting the variables I want\nDBH &lt;- DBH[, c(1,2,5,6,9,11,19,20,22)]\n\n# Changing the possible factor variables into factors\nDBH$HighBP &lt;- factor(DBH$HighBP, levels = c(0, 1), labels = c(\"Not_High\", \"High\"))\nDBH$Smoker &lt;- factor(DBH$Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$PhysActivity &lt;- factor(DBH$PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$Veggies &lt;- factor(DBH$Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$Sex &lt;- factor(DBH$Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\nDBH$Age &lt;- factor(DBH$Age, \n                  levels = 1:13, \n                  labels = c(\"18_24\", \"25_29\", \"30_34\", \"35_39\", \"40_44\", \"45_49\", \n                             \"50_54\", \"55_59\", \"60_64\", \"65_69\", \"70_74\", \"75_79\", \"80_plus\"))\nDBH$Income &lt;- factor(DBH$Income, \n                     levels = 1:8, \n                     labels = c(\"Less_than_10k\", \"10k_to_15k\", \"15k_to_20k\", \n                                \"20k_to_25k\", \"25k_to_35k\", \"35k_to_50k\", \n                                \"50k_to_75k\", \"75k_or_more\"))\nDBH$Diabetes_binary &lt;- factor(DBH$Diabetes_binary, levels = c(0, 1), labels = c(\"No_Diabetes\", \"Diabetes\"))\n\n\n# Checking for N/A values\ncolSums(is.na(DBH))\n\nDiabetes_binary          HighBP             BMI          Smoker    PhysActivity \n              0               0               0               0               0 \n        Veggies             Sex             Age          Income \n              0               0               0               0 \n\n# None found so we are good to move on!"
  },
  {
    "objectID": "EDA.html#summarizations",
    "href": "EDA.html#summarizations",
    "title": "ST 558: Final Project (EDA)",
    "section": "Summarizations",
    "text": "Summarizations\nFirst we will start with summary statistics for our data\n\nsummary(DBH)\n\n    Diabetes_binary        HighBP            BMI        Smoker      \n No_Diabetes:218334   Not_High:144851   Min.   :12.00   No :141257  \n Diabetes   : 35346   High    :108829   1st Qu.:24.00   Yes:112423  \n                                        Median :27.00               \n                                        Mean   :28.38               \n                                        3rd Qu.:31.00               \n                                        Max.   :98.00               \n                                                                    \n PhysActivity Veggies          Sex              Age                Income     \n No : 61760   No : 47839   Female:141974   60_64  :33244   75k_or_more:90385  \n Yes:191920   Yes:205841   Male  :111706   65_69  :32194   50k_to_75k :43219  \n                                           55_59  :30832   35k_to_50k :36470  \n                                           50_54  :26314   25k_to_35k :25883  \n                                           70_74  :23533   20k_to_25k :20135  \n                                           45_49  :19819   15k_to_20k :15994  \n                                           (Other):87744   (Other)    :21594  \n\nby(DBH, DBH$Diabetes_binary, summary)\n\nDBH$Diabetes_binary: No_Diabetes\n    Diabetes_binary        HighBP            BMI        Smoker      \n No_Diabetes:218334   Not_High:136109   Min.   :12.00   No :124228  \n Diabetes   :     0   High    : 82225   1st Qu.:24.00   Yes: 94106  \n                                        Median :27.00               \n                                        Mean   :27.81               \n                                        3rd Qu.:31.00               \n                                        Max.   :98.00               \n                                                                    \n PhysActivity Veggies          Sex              Age                Income     \n No : 48701   No : 39229   Female:123563   60_64  :27511   75k_or_more:83190  \n Yes:169633   Yes:179105   Male  : 94771   55_59  :26569   50k_to_75k :37954  \n                                           65_69  :25636   35k_to_50k :31179  \n                                           50_54  :23226   25k_to_35k :21379  \n                                           70_74  :18392   20k_to_25k :16081  \n                                           45_49  :18077   15k_to_20k :12426  \n                                           (Other):78923   (Other)    :16125  \n------------------------------------------------------------ \nDBH$Diabetes_binary: Diabetes\n    Diabetes_binary       HighBP           BMI        Smoker      PhysActivity\n No_Diabetes:    0   Not_High: 8742   Min.   :13.00   No :17029   No :13059   \n Diabetes   :35346   High    :26604   1st Qu.:27.00   Yes:18317   Yes:22287   \n                                      Median :31.00                           \n                                      Mean   :31.94                           \n                                      3rd Qu.:35.00                           \n                                      Max.   :98.00                           \n                                                                              \n Veggies         Sex             Age               Income    \n No : 8610   Female:18411   65_69  :6558   75k_or_more:7195  \n Yes:26736   Male  :16935   60_64  :5733   35k_to_50k :5291  \n                            70_74  :5141   50k_to_75k :5265  \n                            55_59  :4263   25k_to_35k :4504  \n                            75_79  :3403   20k_to_25k :4054  \n                            80_plus:3209   15k_to_20k :3568  \n                            (Other):7039   (Other)    :5469  \n\n\nNow we will create some visualizations to help better understand our data. First a bar plot to show distribution of patients across the two categories, this gives us a sense of the prevalence of the condition in the dataset.\n\nggplot(DBH, aes(x = Diabetes_binary)) + \n  geom_bar(fill = \"darkblue\") + \n  labs(title = \"Distribution of Diabetes Status\", x = \"Diabetes Status\", y = \"Count\")\n\n\n\n\nNext, a bar plot that demonstrates the proportion of individuals with high blood pressure across the diabetes status. This allows us to observe if high blood pressure is more common in people with or without diabetes.\n\nggplot(DBH, aes(x = Diabetes_binary, fill = HighBP)) + \n  geom_bar(position = \"fill\") + \n  labs(title = \"High Blood Pressure vs Diabetes Status\", x = \"Diabetes Status\", y = \"Proportion\", fill = \"High BP\")\n\n\n\n\nThis box plot shows the distribution of BMI values across the diabetes status, this helps identify if there is a significant difference in BMI across the diabetes variable.\n\nggplot(DBH, aes(x = Diabetes_binary, y = BMI, fill = Diabetes_binary)) +\n  geom_boxplot(outlier.shape = NA) +\n  coord_cartesian(ylim = c(quantile(DBH$BMI, 0.05), quantile(DBH$BMI, 0.95))) +\n  labs(title = \"BMI vs. Diabetes Status\", x = \"Diabetes Status\", y = \"BMI\")\n\n\n\n\nLastly, a bar plot that helps indicate the distribution of age groups across the different diabetes status, it helps reveal if certain age groups are more susceptible to diabetes or not.\n\nggplot(DBH, aes(x = Diabetes_binary, fill = Age)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Age vs. Diabetes Status\", x = \"Diabetes Status\", y = \"Proportion\", fill = \"Age Group\")\n\n\n\n\nSaving as a csv to use for modeling\n\n# Reading in the data\nwrite_csv(DBH, \"preprocessed_diabetes_data.csv\")"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "",
    "section": "",
    "text": "title: “ST 558: Final Project (Modeling)” authors: “Scott Van Slyck” description: “Modeling File for Final Project” date: “July 29, 2024” format: html editor: visual\nLoading/Installing packages necessary ::: {.cell} ::: {.cell-output .cell-output-stderr}\n:::\n:::"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we are working with a dataset from the Diabetes Health Indicators to predict the diabetes status of individuals. The response variable, Diabetes_binary, indicates whether or not a patient has diabetes. Our goal is to create predictive models for the Diabetes_binary variable using various machine learning techniques. We will evaluate the models using log loss as our primary metric.\nFirst we will start by reading in the data and splitting it into training and testing sets. ::: {.cell}\n# Reading in the data using a relative path and manipulating it\n\n# reading in the csv file\nDBH &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# selecting the variables I want\nDBH &lt;- DBH[, c(1,2,5,6,9,11,19,20,22)]\n\n# Changing the possible factor variables into factors\nDBH$HighBP &lt;- factor(DBH$HighBP, levels = c(0, 1), labels = c(\"Not_High\", \"High\"))\nDBH$Smoker &lt;- factor(DBH$Smoker, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$PhysActivity &lt;- factor(DBH$PhysActivity, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$Veggies &lt;- factor(DBH$Veggies, levels = c(0, 1), labels = c(\"No\", \"Yes\"))\nDBH$Sex &lt;- factor(DBH$Sex, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\nDBH$Age &lt;- factor(DBH$Age, \n                  levels = 1:13, \n                  labels = c(\"18_24\", \"25_29\", \"30_34\", \"35_39\", \"40_44\", \"45_49\", \n                             \"50_54\", \"55_59\", \"60_64\", \"65_69\", \"70_74\", \"75_79\", \"80_plus\"))\nDBH$Income &lt;- factor(DBH$Income, \n                     levels = 1:8, \n                     labels = c(\"Less_than_10k\", \"10k_to_15k\", \"15k_to_20k\", \n                                \"20k_to_25k\", \"25k_to_35k\", \"35k_to_50k\", \n                                \"50k_to_75k\", \"75k_or_more\"))\nDBH$Diabetes_binary &lt;- factor(DBH$Diabetes_binary, levels = c(0, 1), labels = c(\"No_Diabetes\", \"Diabetes\"))\n\n# Setting seed and splitting data into training and test sets\nset.seed(270)\n\ntrainIndex &lt;- createDataPartition(DBH$Diabetes_binary, p = 0.7, list = FALSE)\n\ntrainData &lt;- DBH[trainIndex, ]\ntestData &lt;- DBH[-trainIndex, ]\n:::\nLog loss, also known as logistic loss or cross-entropy loss, is a measure of how well a model’s predicted probabilities match the actual outcomes. It evaluates how accurate and confident a model is about it’s prediction, it penalizes wrong predictions with high confidence more than one that is unsure. These penalties result in the model taking more precise and cautious estimates. Where it differs from accuracy is it provides a more detailed view of performance when considering confidence of predictions. This is useful in scenarios where predicting the majority class can give misleadingly high accuracy, focusing on probability estimates allows log loss to help create more reliable models for real-world applications where knowing the likelihood of an outcome is of utmost importance."
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nFirst we will start our modeling with looking at logistic regression models. Logistic regression is a statistical method of modeling to display relationships between a binary response variable and independent variables. Logistic regression’s binary outcome results in the prediction of the probability of the outcome which can be useful for risk assessment. They are easily interpretable due to the coefficients being displayed as log odds of the outcome making it easy to understand the impact of each predictor.\n\n# Control for cross evalution with 3-folds\n\ntrain_control &lt;- trainControl(method = \"cv\", number = 3, classProbs = TRUE, summaryFunction = mnLogLoss)\n\n# Re-set seed\nset.seed(270)\n\n# Model 1: Basic Logistic Regression\nlog_model1 &lt;- train(Diabetes_binary ~ ., data = trainData, method = \"glm\", family = \"binomial\", \n                    trControl = train_control, metric = \"logLoss\")\n\n# Model 2: Logistic Regression with selected predictors\nselected_predictors2 &lt;- c(\"HighBP\", \"BMI\", \"Smoker\", \"PhysActivity\")\nformula2 &lt;- as.formula(paste(\"Diabetes_binary ~\", paste(selected_predictors2, collapse = \" + \")))\n\nlog_model2 &lt;- train(formula2, data = trainData, method = \"glm\", family = \"binomial\", \n                    trControl = train_control, metric = \"logLoss\")\n\n# Model 3: Logistic Regression with another set of selected predictors\nselected_predictors3 &lt;- c(\"Veggies\", \"Sex\", \"Age\", \"Income\")\nformula3 &lt;- as.formula(paste(\"Diabetes_binary ~\", paste(selected_predictors3, collapse = \" + \")))\n\nlog_model3 &lt;- train(formula3, data = trainData, method = \"glm\", family = \"binomial\", \n                    trControl = train_control, metric = \"logLoss\")\n\n# Compare models\nresamples_log &lt;- resamples(list(Full = log_model1, Reduced1 = log_model2, Reduced2 = log_model3))\nsummary(resamples_log)\n\n\nCall:\nsummary.resamples(object = resamples_log)\n\nModels: Full, Reduced1, Reduced2 \nNumber of resamples: 3 \n\nlogLoss \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nFull     0.3378157 0.3380823 0.3383489 0.3382177 0.3384187 0.3384885    0\nReduced1 0.3527385 0.3537711 0.3548036 0.3541612 0.3548726 0.3549416    0\nReduced2 0.3687872 0.3696134 0.3704396 0.3702460 0.3709754 0.3715111    0"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "",
    "section": "Classification Tree",
    "text": "Classification Tree\nA classification tree is a supervised machine learning algorithm used to classify data into certain classes. The classification tree works by splitting the data set into subsets based on the value of a feature which create branches for each possible value. It is particularly useful in this scenario because our response value is a classified variable with either no diabetes or diabetes. This allows the algorithm to select the right features and split on the right values to classify whether or not a patient has diabetes.\n\nset.seed(270)\n\ntree_model &lt;- train(Diabetes_binary ~ ., data = trainData, method = \"rpart\", \n                    trControl = train_control, metric = \"logLoss\", tuneLength = 10)\n\n# Print tree model\nprint(tree_model)\n\nCART \n\n177577 samples\n     8 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118385, 118385, 118384 \nResampling results across tuning parameters:\n\n  cp            logLoss  \n  0.0001414541  0.3621801\n  0.0001443410  0.3621551\n  0.0001616619  0.3604791\n  0.0001886055  0.3600939\n  0.0001905301  0.3600939\n  0.0002020774  0.3600116\n  0.0002424928  0.3599326\n  0.0002829083  0.3597970\n  0.0003031160  0.3743926\n  0.0004261995  0.3892756\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.0002829083."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "",
    "section": "Random Forest",
    "text": "Random Forest\nA random forest is a robust machine learning model that is an improved version of a decision tree in comparison to individual ones. It combines multiple trees, averages them out, and takes randomness for it’s selection of features. This allows for higher accuracy and less bias. It is valuable for both classification and regression learning. The downside is that it is computationally intensive, for the code below I have had to limit the trees it produces to 100 and reduce the 5-fold cross validation to 3-fold CV since the prior was taking more than an hour to run.\n\nset.seed(270)\n\nrf_grid &lt;- expand.grid(mtry = seq(2, ncol(trainData)-1, by = 2))\n\nrf_model &lt;- train(Diabetes_binary ~ ., data = trainData, method = \"rf\", \n                  trControl = train_control, metric = \"logLoss\", tuneGrid = rf_grid, ntree = 100)\n\n# Print random forest model\nprint(rf_model)\n\nRandom Forest \n\n177577 samples\n     8 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118385, 118385, 118384 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  2     3.433080\n  4     2.083126\n  6     1.627539\n  8     1.468988\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 8."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "",
    "section": "Final Model Selection",
    "text": "Final Model Selection\n\n# Ensure testData$Diabetes_binary has the same levels as trainData$Diabetes_binary\ntestData$Diabetes_binary &lt;- factor(testData$Diabetes_binary, levels = levels(trainData$Diabetes_binary))\n\n# Helper function to calculate mnLogLoss with debugging output\ncalculate_log_loss &lt;- function(preds, actual) {\n  data &lt;- data.frame(obs = actual, preds)\n  print(head(data))  # Print the head of the data frame for debugging\n  print(table(data$obs))  # Print a table of the observed values for debugging\n  mnLogLoss(data, lev = levels(actual))\n}\n\n# Predict on the test set using the best logistic regression model\nlog_preds1 &lt;- predict(log_model1, newdata = testData, type = \"prob\")\nprint(head(log_preds1))  # Print the head of the predicted probabilities for debugging\n\n  No_Diabetes    Diabetes\n1   0.8333998 0.166600240\n2   0.9414086 0.058591427\n3   0.9457391 0.054260854\n4   0.9936101 0.006389885\n5   0.5138947 0.486105271\n6   0.8373838 0.162616197\n\nlog_loss_test1 &lt;- calculate_log_loss(log_preds1, testData$Diabetes_binary)\n\n          obs No_Diabetes    Diabetes\n1 No_Diabetes   0.8333998 0.166600240\n2    Diabetes   0.9414086 0.058591427\n3    Diabetes   0.9457391 0.054260854\n4 No_Diabetes   0.9936101 0.006389885\n5 No_Diabetes   0.5138947 0.486105271\n6    Diabetes   0.8373838 0.162616197\n\nNo_Diabetes    Diabetes \n      65500       10603 \n\nlog_preds2 &lt;- predict(log_model2, newdata = testData, type = \"prob\")\nprint(head(log_preds2))  # Print the head of the predicted probabilities for debugging\n\n  No_Diabetes   Diabetes\n1   0.7564907 0.24350934\n2   0.9485347 0.05146526\n3   0.9541127 0.04588734\n4   0.9455279 0.05447213\n5   0.5748257 0.42517430\n6   0.7240746 0.27592540\n\nlog_loss_test2 &lt;- calculate_log_loss(log_preds2, testData$Diabetes_binary)\n\n          obs No_Diabetes   Diabetes\n1 No_Diabetes   0.7564907 0.24350934\n2    Diabetes   0.9485347 0.05146526\n3    Diabetes   0.9541127 0.04588734\n4 No_Diabetes   0.9455279 0.05447213\n5 No_Diabetes   0.5748257 0.42517430\n6    Diabetes   0.7240746 0.27592540\n\nNo_Diabetes    Diabetes \n      65500       10603 \n\nlog_preds3 &lt;- predict(log_model3, newdata = testData, type = \"prob\")\nprint(head(log_preds3))  # Print the head of the predicted probabilities for debugging\n\n  No_Diabetes   Diabetes\n1   0.8936203 0.10637975\n2   0.8885584 0.11144163\n3   0.8353382 0.16466176\n4   0.9895541 0.01044592\n5   0.7807061 0.21929391\n6   0.8950507 0.10494930\n\nlog_loss_test3 &lt;- calculate_log_loss(log_preds3, testData$Diabetes_binary)\n\n          obs No_Diabetes   Diabetes\n1 No_Diabetes   0.8936203 0.10637975\n2    Diabetes   0.8885584 0.11144163\n3    Diabetes   0.8353382 0.16466176\n4 No_Diabetes   0.9895541 0.01044592\n5 No_Diabetes   0.7807061 0.21929391\n6    Diabetes   0.8950507 0.10494930\n\nNo_Diabetes    Diabetes \n      65500       10603 \n\n# Print the log loss values\nprint(c(log_loss_test1, log_loss_test2, log_loss_test3))\n\n  logLoss   logLoss   logLoss \n0.3395125 0.3540067 0.3720823 \n\n# Predict on the test set using the classification tree model\ntree_preds &lt;- predict(tree_model, newdata = testData, type = \"prob\")\nprint(head(tree_preds))  # Print the head of the predicted probabilities for debugging\n\n  No_Diabetes   Diabetes\n1   0.8126694 0.18733064\n2   0.9396545 0.06034551\n3   0.9396545 0.06034551\n4   0.9396545 0.06034551\n5   0.5579632 0.44203684\n6   0.8126694 0.18733064\n\ntree_loss_test &lt;- calculate_log_loss(tree_preds, testData$Diabetes_binary)\n\n          obs No_Diabetes   Diabetes\n1 No_Diabetes   0.8126694 0.18733064\n2    Diabetes   0.9396545 0.06034551\n3    Diabetes   0.9396545 0.06034551\n4 No_Diabetes   0.9396545 0.06034551\n5 No_Diabetes   0.5579632 0.44203684\n6    Diabetes   0.8126694 0.18733064\n\nNo_Diabetes    Diabetes \n      65500       10603 \n\nprint(tree_loss_test)\n\n  logLoss \n0.3607329 \n\n# Predict on the test set using the random forest model\nrf_preds &lt;- predict(rf_model, newdata = testData, type = \"prob\")\nprint(head(rf_preds))  # Print the head of the predicted probabilities for debugging\n\n  No_Diabetes Diabetes\n1        0.94     0.06\n2        1.00     0.00\n3        1.00     0.00\n4        1.00     0.00\n5        0.47     0.53\n6        0.95     0.05\n\nrf_loss_test &lt;- calculate_log_loss(rf_preds, testData$Diabetes_binary)\n\n          obs No_Diabetes Diabetes\n1 No_Diabetes        0.94     0.06\n2    Diabetes        1.00     0.00\n3    Diabetes        1.00     0.00\n4 No_Diabetes        1.00     0.00\n5 No_Diabetes        0.47     0.53\n6    Diabetes        0.95     0.05\n\nNo_Diabetes    Diabetes \n      65500       10603 \n\nprint(rf_loss_test)\n\n logLoss \n1.720237 \n\n# Compare the log loss of all models\nmodel_comparison &lt;- data.frame(\n  Model = c(\"Full Logistic Regression\", \"Reduced Model 1\", \"Reduced Model 2\", \"Classification Tree\", \"Random Forest\"),\n  LogLoss = c(log_loss_test1, log_loss_test2, log_loss_test3, tree_loss_test, rf_loss_test)\n)\nprint(model_comparison)\n\n                     Model   LogLoss\n1 Full Logistic Regression 0.3395125\n2          Reduced Model 1 0.3540067\n3          Reduced Model 2 0.3720823\n4      Classification Tree 0.3607329\n5            Random Forest 1.7202371\n\n\nThe best model, selected by which final model had the lowest logLoss was the full logistic regression model."
  }
]